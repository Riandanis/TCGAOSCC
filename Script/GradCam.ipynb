{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Author:   Kazuto Nakashima\n",
    "# URL:      http://kazuto1011.github.io\n",
    "# Created:  2017-05-26\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim\n",
    "import os\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = '/home/DL-based-Tumor-Classification/Datasets/trial_2nd_8843_true_feature'\n",
    "final_conv = 'conv3'\n",
    "num_epochs = 50\n",
    "batch_size = 400\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    raw_image = cv2.imread(image_path,0)\n",
    "#     raw_image = cv2.resize(raw_image, (96,) * 2)\n",
    "    image = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor()        \n",
    "        ]\n",
    "    )(raw_image[..., ::-1].copy())\n",
    "    image = image.type(torch.DoubleTensor)\n",
    "    return image, raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(cuda):\n",
    "    cuda = cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    if cuda:\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(\"Device:\", torch.cuda.get_device_name(current_device))\n",
    "    else:\n",
    "        print(\"Device: CPU\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop2D): Dropout2d(p=0.25, inplace=False)\n",
       "  (vp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=36864, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_of_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # input image channel, output channels, kernel size square convolution\n",
    "        # kernel\n",
    "        # input size = 102, output size = 100\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # input size = 50, output size = 48\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # input size = 24, output size = 24\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False)\n",
    "        self.vp = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(256*12*12, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, num_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.bn1(self.vp(self.conv1(x))))\n",
    "        x = F.relu(self.bn2(self.vp(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.vp(self.conv3(x))))\n",
    "        x = self.drop2D(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(num_of_classes=2)\n",
    "net = net.double()\n",
    "# net = nn.DataParallel(net)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop2D): Dropout2d(p=0.25, inplace=False)\n",
       "  (vp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=36864, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_path = default_path + '/img_fold8'\n",
    "state_dict = torch.load(def_path + '/network_0505_th1.pth')\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "net.load_state_dict(new_state_dict)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = def_path + '/train/labels_train.csv'\n",
    "train_root_path = def_path + '/train'\n",
    "test_csv_path = def_path + '/test/labels_test.csv'\n",
    "test_root_path = def_path + '/test'\n",
    "test_label_path =  def_path + '/test/test_label_run_th1_test.csv'\n",
    "predicted_label_path = def_path + '/test/predicted_label_run_th1_test.csv'\n",
    "model_path = def_path + '/network_0505_th1.pth'\n",
    "\n",
    "\n",
    "class TumorDatasetTrain(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file , root_dir, transform=None):\n",
    "        self.labels_frame = np.array(pd.read_csv(csv_file, skiprows=1, sep=',', header=None)).astype(np.int)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(idx)+'.png'\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \"\"\"\n",
    "            !!!Pay attention!!!\n",
    "            The image size is set here\n",
    "            \"\"\"\n",
    "        img = np.empty(shape=(1, 96, 96))\n",
    "        img[0, :, :] = (img_as_float(io.imread(img_path)) - 0.5)/0.5\n",
    "        label = np.array([self.labels_frame[idx,1]-1])\n",
    "        train_sample = {'image': img, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            train_sample = self.transform(train_sample)\n",
    "        return train_sample\n",
    "\n",
    "\n",
    "class TumorDatasetTest(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file , root_dir, transform=None):\n",
    "        self.labels_frame = np.array(pd.read_csv(csv_file, skiprows= 1, sep=',', header= None)).astype(np.int)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(idx) + '.png'\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \"\"\"\n",
    "            !!!Pay attention!!!\n",
    "            The image size is set here\n",
    "            \"\"\"\n",
    "        img = np.empty(shape=(1, 96, 96))\n",
    "        img[0, :, :] = (img_as_float(io.imread(img_path)) - 0.5)/0.5\n",
    "        label = np.array([self.labels_frame[idx, 1]-1])\n",
    "        test_sample = {'image': img, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            test_sample = self.transform(test_sample)\n",
    "        return test_sample\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, labels = sample['image'], sample['label']\n",
    "        return {'image': torch.from_numpy(image), 'label': torch.LongTensor(labels)}\n",
    "\n",
    "\n",
    "train_dataset = TumorDatasetTrain(csv_file=train_csv_path, root_dir=train_root_path,\n",
    "                                  transform=transforms.Compose([ToTensor()]))\n",
    "test_dataset = TumorDatasetTest(csv_file=test_csv_path, root_dir=test_root_path, transform=transforms.Compose([ToTensor()]))\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _PropagationBase(object):\n",
    "\n",
    "    def __init__(self, model, cuda=True):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        if cuda:\n",
    "            self.model.cuda()\n",
    "        self.all_fmaps = OrderedDict()\n",
    "        self.all_grads = OrderedDict()\n",
    "        self._set_hook_func()\n",
    "        self.image = None\n",
    "\n",
    "    def _set_hook_func(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _encode_one_hot(self, idx):\n",
    "        one_hot = torch.DoubleTensor(1, self.preds.size()[-1]).zero_()\n",
    "        one_hot[0][idx] = 1.0\n",
    "        return one_hot.cuda()\n",
    "\n",
    "#     def forward(self, image):\n",
    "#         self.image = image\n",
    "#         self.preds = self.model.forward(self.image)\n",
    "#         self.probs = F.softmax(self.preds, dim=1)\n",
    "#         return self.probs.sort(dim = 1, descending= True)\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.image = image\n",
    "        self.preds = self.model.forward(self.image)\n",
    "        self.probs = F.softmax(self.preds, dim=1)[0]\n",
    "        self.prob, self.idx = self.probs.data.sort(0, True)\n",
    "        return self.prob, self.idx\n",
    "\n",
    "    def backward(self, idx):\n",
    "        self.model.zero_grad()\n",
    "        one_hot = self._encode_one_hot(idx)\n",
    "        self.preds.backward(gradient=one_hot, retain_graph=True)\n",
    "        print(\"no error backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM(_PropagationBase):\n",
    "\n",
    "    def _set_hook_func(self):\n",
    "\n",
    "        def func_f(module, input, output):\n",
    "            self.all_fmaps[id(module)] = output.data.cpu()\n",
    "\n",
    "        def func_b(module, grad_in, grad_out):\n",
    "            self.all_grads[id(module)] = grad_out[0].cpu()\n",
    "\n",
    "        for module in self.model.named_modules():\n",
    "            module[1].register_forward_hook(func_f)\n",
    "            module[1].register_backward_hook(func_b)\n",
    "\n",
    "    def _find(self, outputs, target_layer):\n",
    "        for key, value in outputs.items():\n",
    "            for module in self.model.named_modules():\n",
    "                if id(module[1]) == key:\n",
    "                    if module[0] == target_layer:\n",
    "                        return value\n",
    "        raise ValueError('Invalid layer name: {}'.format(target_layer))\n",
    "\n",
    "    def _normalize(self, grads):\n",
    "        l2_norm = torch.sqrt(torch.mean(torch.pow(grads, 2))) + 1e-5\n",
    "        # return grads / l2_norm.data[0]\n",
    "        return grads / l2_norm.data[0]\n",
    "\n",
    "    def _compute_grad_weights(self, grads):\n",
    "        grads = self._normalize(grads)\n",
    "        self.map_size = grads.size()[2:]\n",
    "        return nn.AvgPool2d(self.map_size)(grads)\n",
    "\n",
    "    def generate(self, target_layer):\n",
    "        fmaps = self._find(self.all_fmaps, target_layer)\n",
    "        grads = self._find(self.all_grads, target_layer)\n",
    "        weights = self._compute_grad_weights(grads)\n",
    "        gcam = torch.DoubleTensor(self.map_size).zero_()\n",
    "        for fmap, weight in zip(fmaps[0], weights[0]):\n",
    "            res = fmap * weight.data.expand_as(fmap)\n",
    "            gcam += fmap * weight.data.expand_as(fmap)\n",
    "        gcam = F.relu(Variable(gcam))\n",
    "\n",
    "        gcam = gcam.data.cpu().numpy()\n",
    "        gcam -= gcam.min()\n",
    "        if gcam.max() != 0:\n",
    "            gcam /= gcam.max()\n",
    "        gcam = cv2.resize(gcam, (self.image.size(3), self.image.size(2)))\n",
    "\n",
    "        return gcam\n",
    "\n",
    "    def save(self, filename, gcam, raw_image):\n",
    "        gcam = cv2.applyColorMap(np.uint8(gcam * 255.0), cv2.COLORMAP_JET)\n",
    "        # gcam = gcam.astype(np.float) + raw_image.astype(np.float)\n",
    "        if gcam.max() != 0:\n",
    "            gcam = gcam / gcam.max() * 255.0\n",
    "        cv2.imwrite(filename, np.uint8(gcam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPropagation(_PropagationBase):\n",
    "    def _find(self, outputs, target_layer):\n",
    "        for key, value in outputs.items():\n",
    "            for module in self.model.named_modules():\n",
    "                if id(module[1]) == key:\n",
    "                    if module[0] == target_layer:\n",
    "                        return value\n",
    "        raise ValueError('Invalid layer name: {}'.format(target_layer))\n",
    "\n",
    "    def generate(self):\n",
    "        print('image_size:  ', self.image.size())\n",
    "        print('image_grad:  ', self.image.grad)\n",
    "        output = self.image.grad.data[0].cpu().numpy()[0]\n",
    "        return output\n",
    "\n",
    "    def save(self, filename, data):\n",
    "        abs_max = np.maximum(-1 * data.min(), data.max())\n",
    "        data = data / abs_max * 127.0 + 127.0\n",
    "        cv2.imwrite(filename, np.uint8(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackPropagation(BackPropagation):\n",
    "\n",
    "    def _set_hook_func(self):\n",
    "\n",
    "        def func_b(module, grad_in, grad_out):\n",
    "            print(\"mlaku kene func\")\n",
    "            self.all_grads[id(module)] = grad_in[0]\n",
    "\n",
    "            # Cut off negative gradients\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                print('mlaku po?')\n",
    "                return torch.clamp(grad_in[0], min=0.0)\n",
    "\n",
    "        for module in self.model.named_modules():\n",
    "            module[1].register_backward_hook(func_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii, test_sample in enumerate(dataloader_test):\n",
    "#         test_imgs = Variable(test_sample['image']).cuda()\n",
    "#         print(\"SEE THIS::: \" , test_imgs.shape)\n",
    "#         test_label = Variable(test_sample['label']).cuda()\n",
    "#         print(test_label.shape)\n",
    "device = get_device(True)\n",
    "image_paths = []\n",
    "for filename in os.listdir(def_path + '/test'):\n",
    "    directory = def_path + '/test/'\n",
    "    if filename.endswith('.png'):\n",
    "        image_paths.append(directory + filename)\n",
    "images = []\n",
    "raw_images = []\n",
    "print(\"Images:\")\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    print(\"\\t#{}: {}\".format(i, image_path))\n",
    "    image, raw_image = preprocess(image_path)\n",
    "    images.append(image)\n",
    "    raw_images.append(raw_image)\n",
    "images = torch.stack(images).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 2]) torch.Size([65, 2])\n",
      "ikisingmax  torch.return_types.max(\n",
      "values=tensor([64, 64], device='cuda:0'),\n",
      "indices=tensor([14, 50], device='cuda:0'))\n",
      "ikisingmax  torch.return_types.max(\n",
      "values=tensor([58, 40, 31, 55, 57, 43, 37, 22, 42, 12, 26, 28, 41, 35, 64, 16, 59, 50,\n",
      "        33, 21, 61, 62, 54, 53, 32, 51, 48, 25, 60, 63, 30, 44, 56, 44, 30, 63,\n",
      "        60, 25, 48, 51, 32, 53, 54, 62, 61, 21, 33, 50, 59, 16, 64, 35, 41, 28,\n",
      "        26, 12, 42, 22, 37, 43, 57, 55, 31, 40, 58], device='cuda:0'),\n",
      "indices=tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0], device='cuda:0'))\n",
      "ikiids tensor([ 1, 40, 31, 39, 57, 43, 19, 22, 17,  8, 26, 28, 41, 14, 64,  9, 59, 50,\n",
      "         2, 11, 18, 49, 54, 53,  4, 51, 10,  3, 52, 63, 30, 44, 56, 24, 29, 47,\n",
      "        60, 25, 48, 45, 32, 38, 34, 62, 61, 21, 33, 36, 46, 16, 27, 35, 13,  7,\n",
      "         5, 12, 42,  0, 37, 15, 23, 55, 20,  6, 58], device='cuda:0')\n",
      "tensor([[0., 0.]], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 40 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-798ca203ab1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ikiids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-829752845e22>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-829752845e22>\u001b[0m in \u001b[0;36m_encode_one_hot\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mone_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 40 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "gcam = GradCAM(model=net)\n",
    "probs, ids = gcam.forward(images)\n",
    "\n",
    "print(probs.shape , ids.shape)\n",
    "#gbackprop\n",
    "gbp = GuidedBackPropagation(model = net)\n",
    "_ = gbp.forward(images)\n",
    "print(\"ikisingmax \",torch.max(ids, 0))\n",
    "print(\"ikisingmax \",torch.max(ids, 1))\n",
    "for i in range(2):\n",
    "    print(\"ikiids\", ids[:,i])\n",
    "    gbp.backward(idx=ids[:, [i]])\n",
    "    gradients = gbp.generate()\n",
    "\n",
    "    gcam.backward(idx=ids[:, [i]])\n",
    "    regions = gcam.generate(target_layer = 'conv3')\n",
    "\n",
    "    for i in range(len(images)):\n",
    "#         features_blobs = []\n",
    "#         print(\"now in loop \"+str(i)+\"!!!!!\")\n",
    "#         test_imgs_temp = test_imgs[i,:,:,:]\n",
    "#         test_imgs_in = test_imgs_temp.unsqueeze(0)\n",
    "#         test_label_temp = test_label[i,:]\n",
    "#         test_label_in = test_label_temp.unsqueeze(0)\n",
    "        gbp.save(filename = def_path + '/gradcamcoba/gbp/' + j +'_gbp.png', data = gradients[j])\n",
    "        gcam.save(filename = def_path + '/gradcamcoba/gcam/' + j +'_gcam.png', gcam = regions[j,0], raw_image = raw_images[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now in loop 0!!!!!\n",
      "ikiids tensor([0, 1], device='cuda:0')\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "mlaku kene func\n",
      "no error backward\n",
      "image_size:   torch.Size([1, 1, 96, 96])\n",
      "image_grad:   None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2e6f8b61a342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ikiids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mgbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mgcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1819c529da76>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_size:  '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_grad:  '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "#gcam\n",
    "for ii, test_sample in enumerate(dataloader_test):\n",
    "        test_imgs = Variable(test_sample['image']).cuda()\n",
    "        test_label = Variable(test_sample['label']).cuda()\n",
    "#         label_coba = test_label.view(-1)\n",
    "        \n",
    "#         bp = BackPropagation(model=net)\n",
    "#         probs, ids = bp.forwards(test_imgs)\n",
    "        \n",
    "       \n",
    "#         print(probs)\n",
    "#         print(\"ikisingmax \",torch.max(ids, 0))\n",
    "#         print(\"ikisingmax \",torch.max(ids, 1))\n",
    "        \n",
    "        for j in range(0,test_imgs.size(0)):\n",
    "            features_blobs = []\n",
    "            print(\"now in loop \"+str(j)+\"!!!!!\")\n",
    "            test_imgs_temp = test_imgs[j,:,:,:]\n",
    "            test_imgs_in = test_imgs_temp.unsqueeze(0)\n",
    "            test_label_temp = test_label[j,:]\n",
    "            test_label_in = test_label_temp.unsqueeze(0)\n",
    "            \n",
    "            gcam = GradCAM(model=net)\n",
    "            probs, ids = gcam.forward(test_imgs_in)\n",
    "            #gbackprop\n",
    "            gbp = GuidedBackPropagation(model = net)\n",
    "            probs, ids = gbp.forward(test_imgs_in)\n",
    "            \n",
    "            for i in range(0,2):\n",
    "                print(\"ikiids\", ids)\n",
    "                gbp.backward(idx=ids[i])\n",
    "                gradients = gbp.generate()\n",
    "\n",
    "                gcam.backward(idx=ids[:, [i]])\n",
    "                regions = gcam.generate(target_layer = 'conv3')\n",
    "            \n",
    "            gbp.save(filename = def_path + '/gradcamcoba/gbp/' + j +'_gbp.png', data = gradients[j])\n",
    "            gcam.save(filename = def_path + '/gradcamcoba/gcam/' + j +'_gcam.png', gcam = regions[j,0], raw_image = test_imgs_in)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
