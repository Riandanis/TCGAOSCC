{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "# Author:   Kazuto Nakashima\n",
    "# URL:      http://kazuto1011.github.io\n",
    "# Created:  2017-05-26\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim\n",
    "import os\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_of_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # input image channel, output channels, kernel size square convolution\n",
    "        # kernel\n",
    "        # input size = 102, output size = 100\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # input size = 50, output size = 48\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # input size = 24, output size = 24\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False)\n",
    "        self.vp = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(256*12*12, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, num_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.bn1(self.vp(self.conv1(x))))\n",
    "        x = F.relu(self.bn2(self.vp(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.vp(self.conv3(x))))\n",
    "        x = self.drop2D(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(num_of_classes=2)\n",
    "net = net.double()\n",
    "# net = nn.DataParallel(net)\n",
    "net.cuda()\n",
    "\n",
    "\n",
    "class _BaseWrapper(object):\n",
    "    def __init__(self, model):\n",
    "        super(_BaseWrapper, self).__init__()\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.model = model\n",
    "        self.handlers = []  # a set of hook function handlers\n",
    "\n",
    "    def _encode_one_hot(self, ids):\n",
    "        one_hot = torch.zeros_like(self.logits).to(self.device)\n",
    "        one_hot.scatter_(1, ids, 1.0)\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.image_shape = image.shape[2:]\n",
    "        self.logits = self.model(image)\n",
    "        self.probs = F.softmax(self.logits, dim=1)\n",
    "        return self.probs.sort(dim=1, descending=True)  # ordered results\n",
    "\n",
    "    def backward(self, ids):\n",
    "        \"\"\"\n",
    "        Class-specific backpropagation\n",
    "        \"\"\"\n",
    "        one_hot = self._encode_one_hot(ids)\n",
    "        self.model.zero_grad()\n",
    "        self.logits.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "    def generate(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def remove_hook(self):\n",
    "        \"\"\"\n",
    "        Remove all the forward/backward hook functions\n",
    "        \"\"\"\n",
    "        for handle in self.handlers:\n",
    "            handle.remove()\n",
    "\n",
    "\n",
    "class BackPropagation(_BaseWrapper):\n",
    "    def forward(self, image):\n",
    "        self.image = image.requires_grad_()\n",
    "        return super(BackPropagation, self).forward(self.image)\n",
    "\n",
    "    def generate(self):\n",
    "        gradient = self.image.grad.clone()\n",
    "        self.image.grad.zero_()\n",
    "        return gradient\n",
    "\n",
    "\n",
    "class GuidedBackPropagation(BackPropagation):\n",
    "    \"\"\"\n",
    "    \"Striving for Simplicity: the All Convolutional Net\"\n",
    "    https://arxiv.org/pdf/1412.6806.pdf\n",
    "    Look at Figure 1 on page 8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackPropagation, self).__init__(model)\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # Cut off negative gradients\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                return (F.relu(grad_in[0]),)\n",
    "\n",
    "        for module in self.model.named_modules():\n",
    "            self.handlers.append(module[1].register_backward_hook(backward_hook))\n",
    "\n",
    "\n",
    "class GradCAM(PropagationBase):\n",
    "\n",
    "    def _set_hook_func(self):\n",
    "\n",
    "        def func_f(module, input, output):\n",
    "            self.all_fmaps[id(module)] = output.data.cpu()\n",
    "\n",
    "        def func_b(module, grad_in, grad_out):\n",
    "            self.all_grads[id(module)] = grad_out[0].cpu()\n",
    "\n",
    "        for module in self.model.named_modules():\n",
    "            module[1].register_forward_hook(func_f)\n",
    "            module[1].register_backward_hook(func_b)\n",
    "\n",
    "    def _find(self, outputs, target_layer):\n",
    "        for key, value in outputs.items():\n",
    "            for module in self.model.named_modules():\n",
    "                if id(module[1]) == key:\n",
    "                    if module[0] == target_layer:\n",
    "                        return value\n",
    "        raise ValueError('Invalid layer name: {}'.format(target_layer))\n",
    "\n",
    "    def _normalize(self, grads):\n",
    "        l2_norm = torch.sqrt(torch.mean(torch.pow(grads, 2))) + 1e-5\n",
    "        # return grads / l2_norm.data[0]\n",
    "        return grads / l2_norm.data[0]\n",
    "\n",
    "    def _compute_grad_weights(self, grads):\n",
    "        grads = self._normalize(grads)\n",
    "        self.map_size = grads.size()[2:]\n",
    "        return nn.AvgPool2d(self.map_size)(grads)\n",
    "\n",
    "    def generate(self, target_layer):\n",
    "        fmaps = self._find(self.all_fmaps, target_layer)\n",
    "        grads = self._find(self.all_grads, target_layer)\n",
    "        weights = self._compute_grad_weights(grads)\n",
    "        gcam = torch.DoubleTensor(self.map_size).zero_()\n",
    "        for fmap, weight in zip(fmaps[0], weights[0]):\n",
    "            res = fmap * weight.data.expand_as(fmap)\n",
    "            gcam += fmap * weight.data.expand_as(fmap)\n",
    "        gcam = F.relu(Variable(gcam))\n",
    "\n",
    "        gcam = gcam.data.cpu().numpy()\n",
    "        gcam -= gcam.min()\n",
    "        if gcam.max() != 0:\n",
    "            gcam /= gcam.max()\n",
    "        gcam = cv2.resize(gcam, (self.image.size(3), self.image.size(2)))\n",
    "\n",
    "        return gcam\n",
    "\n",
    "    def save(self, filename, gcam, raw_image):\n",
    "        gcam = cv2.applyColorMap(np.uint8(gcam * 255.0), cv2.COLORMAP_JET)\n",
    "        # gcam = gcam.astype(np.float) + raw_image.astype(np.float)\n",
    "        if gcam.max() != 0:\n",
    "            gcam = gcam / gcam.max() * 255.0\n",
    "        cv2.imwrite(filename, np.uint8(gcam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_path = default_path + '/img_fold8'\n",
    "state_dict = torch.load(def_path + '/network_0505_th1.pth')\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "net.load_state_dict(new_state_dict)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = BackPropagation(model=net)\n",
    "    probs, ids = bp.forward(images)  # sorted\n",
    "\n",
    "    for i in range(2):\n",
    "        bp.backward(ids=ids[:, [i]])\n",
    "        gradients = bp.generate()\n",
    "\n",
    "        # Save results as image files\n",
    "        for j in range(len(images)):\n",
    "            print(\"\\t#{}: {} ({:.5f})\".format(j, classes[ids[j, i]], probs[j, i]))\n",
    "\n",
    "            save_gradient(\n",
    "                filename=osp.join(\n",
    "                    output_dir,\n",
    "                    \"{}-{}-vanilla-{}.png\".format(j, arch, classes[ids[j, i]]),\n",
    "                ),\n",
    "                gradient=gradients[j],\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
