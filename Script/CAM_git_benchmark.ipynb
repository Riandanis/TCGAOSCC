{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import cv2, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from skimage import img_as_float,io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path= '/home/DL-based-Tumor-Classification/Data_mapped_images'\n",
    "final_conv = 'conv3'\n",
    "num_epochs = 50\n",
    "batch_size = 400\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop2D): Dropout2d(p=0.25, inplace=False)\n",
       "  (vp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=36864, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=33, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_of_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # input image channel, output channels, kernel size square convolution\n",
    "        # kernel\n",
    "        # input size = 102, output size = 100\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # input size = 50, output size = 48\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # input size = 24, output size = 24\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False)\n",
    "        self.vp = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(256*12*12, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, num_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.bn1(self.vp(self.conv1(x))))\n",
    "        x = F.relu(self.bn2(self.vp(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.vp(self.conv3(x))))\n",
    "        x = self.drop2D(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(num_of_classes=33)\n",
    "net = net.double()\n",
    "# net = nn.DataParallel(net)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pretrained Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"fc4.weight\", \"fc4.bias\". \n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([128, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([33, 512]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([33]) from checkpoint, the shape in current model is torch.Size([256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e79380bfdec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnew_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# load params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 845\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"fc4.weight\", \"fc4.bias\". \n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([128, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([33, 512]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([33]) from checkpoint, the shape in current model is torch.Size([256])."
     ]
    }
   ],
   "source": [
    "def_path = default_path + '/img_fold8'\n",
    "state_dict = torch.load(def_path + '/network_0505_th1.pth')\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "net.load_state_dict(new_state_dict)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = def_path + '/train/labels_train.csv'\n",
    "train_root_path = def_path + '/train'\n",
    "test_csv_path = def_path + '/test/labels_test.csv'\n",
    "test_root_path = def_path + '/test'\n",
    "test_label_path =  def_path + '/test/test_label_run_th1_test.csv'\n",
    "predicted_label_path = def_path + '/test/predicted_label_run_th1_test.csv'\n",
    "model_path = def_path + '/network_0505_th1.pth'\n",
    "\n",
    "\n",
    "class TumorDatasetTrain(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file , root_dir, transform=None):\n",
    "        self.labels_frame = np.array(pd.read_csv(csv_file, skiprows=1, sep=',', header=None)).astype(np.int)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(idx)+'.png'\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \"\"\"\n",
    "            !!!Pay attention!!!\n",
    "            The image size is set here\n",
    "            \"\"\"\n",
    "        img = np.empty(shape=(1, 102, 102))\n",
    "        img[0, :, :] = (img_as_float(io.imread(img_path)) - 0.5)/0.5\n",
    "        label = np.array([self.labels_frame[idx,1]-1])\n",
    "        train_sample = {'image': img, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            train_sample = self.transform(train_sample)\n",
    "        return train_sample\n",
    "\n",
    "\n",
    "class TumorDatasetTest(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file , root_dir, transform=None):\n",
    "        self.labels_frame = np.array(pd.read_csv(csv_file, skiprows= 1, sep=',', header= None)).astype(np.int)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(idx) + '.png'\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \"\"\"\n",
    "            !!!Pay attention!!!\n",
    "            The image size is set here\n",
    "            \"\"\"\n",
    "        img = np.empty(shape=(1, 102, 102))\n",
    "        img[0, :, :] = (img_as_float(io.imread(img_path)) - 0.5)/0.5\n",
    "        label = np.array([self.labels_frame[idx, 1]-1])\n",
    "        test_sample = {'image': img, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            test_sample = self.transform(test_sample)\n",
    "        return test_sample\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, labels = sample['image'], sample['label']\n",
    "        return {'image': torch.from_numpy(image), 'label': torch.LongTensor(labels)}\n",
    "\n",
    "\n",
    "train_dataset = TumorDatasetTrain(csv_file=train_csv_path, root_dir=train_root_path,\n",
    "                                  transform=transforms.Compose([ToTensor()]))\n",
    "test_dataset = TumorDatasetTest(csv_file=test_csv_path, root_dir=test_root_path, transform=transforms.Compose([ToTensor()]))\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate class activation mapping for the top1 prediction\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    \n",
    "#     print(feature_conv.shape)\n",
    "    \n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "def get_cam(net, features_blobs, img_pil, classes, root_img, item):\n",
    "    params = list(net.parameters())\n",
    "    param_temp = np.array(params)\n",
    "    \n",
    "#     print(param_temp.shape)\n",
    "    \n",
    "    weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "    \n",
    "#     print('weight softmax')\n",
    "#     print(weight_softmax.shape)\n",
    "    \n",
    "    normalize =  transforms.Normalize([0.5], [0.5])\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "#     img = np.empty(shape=(1, 102, 102))\n",
    "#     img[0, :, :] = (img_as_float(io.imread(root_img)) - 0.5)/0.5\n",
    "#     print(img.shape)\n",
    "#     img_tensor = preprocess(img_pil)\n",
    "#     img_variable = Variable(img_tensor).cuda()\n",
    "    \n",
    "    outputs = net(img_pil)\n",
    "    \n",
    "#     print('outputs')\n",
    "#     print(outputs.shape)\n",
    "    \n",
    "    probs, idx = torch.max(outputs.data, 1)\n",
    "    \n",
    "#     print('probs')\n",
    "#     print(probs.shape)\n",
    "#     print('idx')\n",
    "#     print(idx.shape)\n",
    "    \n",
    "    \n",
    "    h_x = F.softmax(outputs, dim=1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    \n",
    "#     print(\"probs: \" )\n",
    "#     print('idx: ')\n",
    "#     print(\"before numpy\")\n",
    "#     print(probs.shape)\n",
    "#     print(idx.shape)\n",
    "\n",
    "    \n",
    "    probs = probs.cpu().numpy()\n",
    "    idx = idx.cpu().numpy()\n",
    "    \n",
    "#     print(\"after numpy\")\n",
    "#     print(probs.shape)\n",
    "#     print(idx.shape)\n",
    "\n",
    "#     print('idx 0:' + str(idx[0]))\n",
    "#     print(len(features_blobs))\n",
    "\n",
    "\n",
    "    # output: the prediction\n",
    "#     line = '{:.3f} -> {}'.format(probs[0], classes[idx[0]])\n",
    "#     print(line)\n",
    "\n",
    "    CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "    # render the CAM and output\n",
    "#     print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0]])\n",
    "    img = cv2.imread(root_img)\n",
    "    height, width, _ = img.shape\n",
    "    CAM = cv2.resize(CAMs[0], (width, height))\n",
    "    heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "    result = heatmap * 0.3 + img * 0.5\n",
    "    cv2.imwrite(default_path+ '/img_fold8' + '/coba4' + '/'+str(idx[0])+'_Cam_'+str(item)+'.jpg', result)\n",
    "    print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEE THIS:::  torch.Size([65, 1, 96, 96])\n",
      "torch.Size([65, 1])\n",
      "now in loop 0!!!!!\n",
      "DONE!\n",
      "now in loop 1!!!!!\n",
      "DONE!\n",
      "now in loop 2!!!!!\n",
      "DONE!\n",
      "now in loop 3!!!!!\n",
      "DONE!\n",
      "now in loop 4!!!!!\n",
      "DONE!\n",
      "now in loop 5!!!!!\n",
      "DONE!\n",
      "now in loop 6!!!!!\n",
      "DONE!\n",
      "now in loop 7!!!!!\n",
      "DONE!\n",
      "now in loop 8!!!!!\n",
      "DONE!\n",
      "now in loop 9!!!!!\n",
      "DONE!\n",
      "now in loop 10!!!!!\n",
      "DONE!\n",
      "now in loop 11!!!!!\n",
      "DONE!\n",
      "now in loop 12!!!!!\n",
      "DONE!\n",
      "now in loop 13!!!!!\n",
      "DONE!\n",
      "now in loop 14!!!!!\n",
      "DONE!\n",
      "now in loop 15!!!!!\n",
      "DONE!\n",
      "now in loop 16!!!!!\n",
      "DONE!\n",
      "now in loop 17!!!!!\n",
      "DONE!\n",
      "now in loop 18!!!!!\n",
      "DONE!\n",
      "now in loop 19!!!!!\n",
      "DONE!\n",
      "now in loop 20!!!!!\n",
      "DONE!\n",
      "now in loop 21!!!!!\n",
      "DONE!\n",
      "now in loop 22!!!!!\n",
      "DONE!\n",
      "now in loop 23!!!!!\n",
      "DONE!\n",
      "now in loop 24!!!!!\n",
      "DONE!\n",
      "now in loop 25!!!!!\n",
      "DONE!\n",
      "now in loop 26!!!!!\n",
      "DONE!\n",
      "now in loop 27!!!!!\n",
      "DONE!\n",
      "now in loop 28!!!!!\n",
      "DONE!\n",
      "now in loop 29!!!!!\n",
      "DONE!\n",
      "now in loop 30!!!!!\n",
      "DONE!\n",
      "now in loop 31!!!!!\n",
      "DONE!\n",
      "now in loop 32!!!!!\n",
      "DONE!\n",
      "now in loop 33!!!!!\n",
      "DONE!\n",
      "now in loop 34!!!!!\n",
      "DONE!\n",
      "now in loop 35!!!!!\n",
      "DONE!\n",
      "now in loop 36!!!!!\n",
      "DONE!\n",
      "now in loop 37!!!!!\n",
      "DONE!\n",
      "now in loop 38!!!!!\n",
      "DONE!\n",
      "now in loop 39!!!!!\n",
      "DONE!\n",
      "now in loop 40!!!!!\n",
      "DONE!\n",
      "now in loop 41!!!!!\n",
      "DONE!\n",
      "now in loop 42!!!!!\n",
      "DONE!\n",
      "now in loop 43!!!!!\n",
      "DONE!\n",
      "now in loop 44!!!!!\n",
      "DONE!\n",
      "now in loop 45!!!!!\n",
      "DONE!\n",
      "now in loop 46!!!!!\n",
      "DONE!\n",
      "now in loop 47!!!!!\n",
      "DONE!\n",
      "now in loop 48!!!!!\n",
      "DONE!\n",
      "now in loop 49!!!!!\n",
      "DONE!\n",
      "now in loop 50!!!!!\n",
      "DONE!\n",
      "now in loop 51!!!!!\n",
      "DONE!\n",
      "now in loop 52!!!!!\n",
      "DONE!\n",
      "now in loop 53!!!!!\n",
      "DONE!\n",
      "now in loop 54!!!!!\n",
      "DONE!\n",
      "now in loop 55!!!!!\n",
      "DONE!\n",
      "now in loop 56!!!!!\n",
      "DONE!\n",
      "now in loop 57!!!!!\n",
      "DONE!\n",
      "now in loop 58!!!!!\n",
      "DONE!\n",
      "now in loop 59!!!!!\n",
      "DONE!\n",
      "now in loop 60!!!!!\n",
      "DONE!\n",
      "now in loop 61!!!!!\n",
      "DONE!\n",
      "now in loop 62!!!!!\n",
      "DONE!\n",
      "now in loop 63!!!!!\n",
      "DONE!\n",
      "now in loop 64!!!!!\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "features_blobs = []\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(final_conv).register_forward_hook(hook_feature)\n",
    "\n",
    "for ii, test_sample in enumerate(dataloader_test):\n",
    "        test_imgs = Variable(test_sample['image']).cuda()\n",
    "        print(\"SEE THIS::: \" , test_imgs.shape)\n",
    "        test_label = Variable(test_sample['label']).cuda()\n",
    "        print(test_label.shape)\n",
    "        root = test_root_path + '/' + str(ii) + '.png'\n",
    "        for i in range(0,test_imgs.size(0)):\n",
    "            features_blobs = []\n",
    "            print(\"now in loop \"+str(i)+\"!!!!!\")\n",
    "            test_imgs_temp = test_imgs[i,:,:,:]\n",
    "            test_imgs_in = test_imgs_temp.unsqueeze(0)\n",
    "            test_label_temp = test_label[i,:]\n",
    "            test_label_in = test_label_temp.unsqueeze(0)\n",
    "            get_cam(net, features_blobs, test_imgs_in, test_label_in, root, i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "classes = {0: 'oral', 1: 'cervical'}\n",
    "features_blobs = []\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(final_conv).register_forward_hook(hook_feature)\n",
    "\n",
    "root = '/home/DL-based-Tumor-Classification/model/utils/37.png'\n",
    "# test_imgs = Image.open(root)\n",
    "\n",
    "\n",
    "for ii, test_sample in enumerate(dataloader_test):\n",
    "        test_imgs = Variable(test_sample['image']).cuda()\n",
    "        print(\"SEE THIS::: \" , test_imgs.shape)\n",
    "        test_label = Variable(test_sample['label']).cuda()\n",
    "        print(test_label.shape)\n",
    "        root = test_root_path + '/' + str(ii) + '.png'\n",
    "        get_cam(net, features_blobs, test_imgs, test_label, root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 1])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 1])\n",
      "tensor([[0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(test_label.size())\n",
    "imgs = test_label[0,:]\n",
    "print(imgs.shape)\n",
    "print(imgs.unsqueeze(0).size())\n",
    "unsq = imgs.unsqueeze(0)\n",
    "print(unsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(test_imgs.size(0)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
